
---

### 📚 Fundamental ML Terms

1. **Training Set** – The portion of data used to train a machine learning model.

2. **Training Instance (Sample)** – A single data point (row) from the training set.

3. **Model** – A mathematical structure or algorithm that makes predictions or decisions based on data.

4. **Training Data** – The data (features + labels) used to fit the model.

5. **Accuracy** – The ratio of correct predictions to total predictions in classification tasks.

6. **Data Mining** – Discovering patterns and insights from large datasets, often used in business and analytics.

7. **Labels** – The target values or outcomes the model is trying to predict (e.g., spam/not spam).

8. **Classification** – A supervised learning task where the goal is to predict class labels.

9. **Regression** – A supervised learning task where the goal is to predict continuous numeric values.

10. **Predictors (Attributes/Features)** – The input variables used to make predictions.

---

### 🔍 Unsupervised & Specialized Learning

11. **Clustering** – Grouping similar data points without labels (unsupervised).

12. **Dimensionality Reduction** – Reducing the number of input features while preserving important structure.

13. **Feature Extraction** – Transforming raw data into numerical features usable by models.

14. **Anomaly Detection** – Identifying rare or unusual data points that deviate from the norm.

15. **Novelty Detection** – Detecting new patterns that were not seen during training.

16. **Transfer Learning** – Reusing a model trained on one task as a starting point for a different but related task.

17. **Policy** – A decision-making function in reinforcement learning that maps states to actions.

---

### ⚙️ Learning Methods & Modes

18. **Offline Learning** – The model is trained once on the full dataset and then deployed.

19. **Model Rot (Data Drift)** – The model’s performance degrades over time due to changes in data distribution.

20. **Online Learning** – The model learns continuously from incoming data streams.

21. **Out-of-Core Learning** – Training models on data that’s too large to fit into memory, by streaming data in batches.

22. **Learning Rate** – A hyperparameter that controls how much the model updates with each training step.

23. **Instance-Based Learning** – The model memorizes training instances and compares new inputs to them (e.g., k-NN).

24. **Model-Based Learning** – The model learns a set of parameters to generalize from the data (e.g., linear regression).

---

### ⚖️ Optimization & Evaluation

25. **Utility Function (Fitness Function)** – Measures how good a solution or model is in optimization tasks.

26. **Cost Function (Loss Function)** – Measures how wrong the model’s predictions are; used during training to improve the model.

27. **Training** – The process of fitting a model to the training data by minimizing the cost function.

28. **Inference** – Using a trained model to make predictions on new, unseen data.

29. **Sampling Noise** – Random variability in the data due to small or unrepresentative samples.

30. **Sampling Bias** – A systematic error where the data sample doesn’t reflect the real-world population.

---

### 🔧 Feature Handling & Engineering

31. **Feature Selection** – Choosing the most relevant features from the dataset.

32. **Feature Extraction** – Deriving new, informative features from raw data (e.g., using PCA or CNNs).

33. **Feature Engineering** – Creating new features using domain knowledge to improve model performance.

---

### ⚠️ Model Performance Concepts

34. **Overfitting** – The model learns noise or details in training data that don’t generalize well.

35. **Regularization** – Techniques to reduce overfitting by penalizing overly complex models.

36. **Hyperparameter** – Configuration settings set before training (e.g., learning rate, tree depth).

37. **Underfitting** – The model is too simple to capture the underlying structure of the data.

38. **Generalization Error** – The difference between model performance on training data vs. new data.

---

### 🧪 Validation Methods

39. **Holdout Validation** – Splitting the dataset into training and test sets to evaluate model performance.

40. **Development Set (Validation Set)** – A subset of data used to tune model hyperparameters.

41. **Cross-Validation** – Repeatedly training and validating the model on different data folds for robust evaluation.

---
