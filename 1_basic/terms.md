
---

### ğŸ“š Fundamental ML Terms

1. **Training Set** â€“ The portion of data used to train a machine learning model.

2. **Training Instance (Sample)** â€“ A single data point (row) from the training set.

3. **Model** â€“ A mathematical structure or algorithm that makes predictions or decisions based on data.

4. **Training Data** â€“ The data (features + labels) used to fit the model.

5. **Accuracy** â€“ The ratio of correct predictions to total predictions in classification tasks.

6. **Data Mining** â€“ Discovering patterns and insights from large datasets, often used in business and analytics.

7. **Labels** â€“ The target values or outcomes the model is trying to predict (e.g., spam/not spam).

8. **Classification** â€“ A supervised learning task where the goal is to predict class labels.

9. **Regression** â€“ A supervised learning task where the goal is to predict continuous numeric values.

10. **Predictors (Attributes/Features)** â€“ The input variables used to make predictions.

---

### ğŸ” Unsupervised & Specialized Learning

11. **Clustering** â€“ Grouping similar data points without labels (unsupervised).

12. **Dimensionality Reduction** â€“ Reducing the number of input features while preserving important structure.

13. **Feature Extraction** â€“ Transforming raw data into numerical features usable by models.

14. **Anomaly Detection** â€“ Identifying rare or unusual data points that deviate from the norm.

15. **Novelty Detection** â€“ Detecting new patterns that were not seen during training.

16. **Transfer Learning** â€“ Reusing a model trained on one task as a starting point for a different but related task.

17. **Policy** â€“ A decision-making function in reinforcement learning that maps states to actions.

---

### âš™ï¸ Learning Methods & Modes

18. **Offline Learning** â€“ The model is trained once on the full dataset and then deployed.

19. **Model Rot (Data Drift)** â€“ The modelâ€™s performance degrades over time due to changes in data distribution.

20. **Online Learning** â€“ The model learns continuously from incoming data streams.

21. **Out-of-Core Learning** â€“ Training models on data thatâ€™s too large to fit into memory, by streaming data in batches.

22. **Learning Rate** â€“ A hyperparameter that controls how much the model updates with each training step.

23. **Instance-Based Learning** â€“ The model memorizes training instances and compares new inputs to them (e.g., k-NN).

24. **Model-Based Learning** â€“ The model learns a set of parameters to generalize from the data (e.g., linear regression).

---

### âš–ï¸ Optimization & Evaluation

25. **Utility Function (Fitness Function)** â€“ Measures how good a solution or model is in optimization tasks.

26. **Cost Function (Loss Function)** â€“ Measures how wrong the modelâ€™s predictions are; used during training to improve the model.

27. **Training** â€“ The process of fitting a model to the training data by minimizing the cost function.

28. **Inference** â€“ Using a trained model to make predictions on new, unseen data.

29. **Sampling Noise** â€“ Random variability in the data due to small or unrepresentative samples.

30. **Sampling Bias** â€“ A systematic error where the data sample doesnâ€™t reflect the real-world population.

---

### ğŸ”§ Feature Handling & Engineering

31. **Feature Selection** â€“ Choosing the most relevant features from the dataset.

32. **Feature Extraction** â€“ Deriving new, informative features from raw data (e.g., using PCA or CNNs).

33. **Feature Engineering** â€“ Creating new features using domain knowledge to improve model performance.

---

### âš ï¸ Model Performance Concepts

34. **Overfitting** â€“ The model learns noise or details in training data that donâ€™t generalize well.

35. **Regularization** â€“ Techniques to reduce overfitting by penalizing overly complex models.

36. **Hyperparameter** â€“ Configuration settings set before training (e.g., learning rate, tree depth).

37. **Underfitting** â€“ The model is too simple to capture the underlying structure of the data.

38. **Generalization Error** â€“ The difference between model performance on training data vs. new data.

---

### ğŸ§ª Validation Methods

39. **Holdout Validation** â€“ Splitting the dataset into training and test sets to evaluate model performance.

40. **Development Set (Validation Set)** â€“ A subset of data used to tune model hyperparameters.

41. **Cross-Validation** â€“ Repeatedly training and validating the model on different data folds for robust evaluation.

---
